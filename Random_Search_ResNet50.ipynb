{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Training and Validation of the Random Search Hyperparameter Optimized ResNet-50 Model on the Celeb-DF (Version 1) Dataset\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load saved model\n",
    "loaded_model = joblib.load('/content/drive/MyDrive/model_filename_original_resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import joblib \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Defines batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Defines the base folder path to utilized dataset\n",
    "base_path = '/content/drive/MyDrive/Celeb-DFF'\n",
    "\n",
    "# Defines subfolder paths for real and fake videos\n",
    "real_path_1 = os.path.join(base_path, 'Celeb-real')\n",
    "fake_path = os.path.join(base_path, 'Celeb-synthesis')\n",
    "\n",
    "# File paths for real and fake videos\n",
    "fake_video_files = [os.path.join(fake_path, file) for file in os.listdir(fake_path) if file.endswith('.mp4')]\n",
    "real_video_files = [os.path.join(real_path_1, file) for file in os.listdir(real_path_1) if file.endswith('.mp4')]\n",
    "\n",
    "# Creates labels\n",
    "real_labels = ['echt'] * len(real_video_files)\n",
    "fake_labels = ['df'] * len(fake_video_files)\n",
    "\n",
    "# Combines file paths and labels\n",
    "file_paths = real_video_files + fake_video_files\n",
    "labels = real_labels + fake_labels\n",
    "\n",
    "# Splits the dataset into training (80%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Loads the pretrained ResNet-50 model with weights from ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Adds a global average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Creates the final feature extraction model\n",
    "feature_extraction_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "\n",
    "def load_and_preprocess_video(file_path, label, desired_num_frames=16):\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    frames = []\n",
    "\n",
    "   \n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for _ in tqdm(range(min(num_frames, desired_num_frames)), desc=f\"Processing {os.path.basename(file_path)}\", unit=\" frame\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "      \n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  \n",
    "        frame = np.expand_dims(frame, axis=0)  \n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "  \n",
    "    if not frames:\n",
    "        return None, label\n",
    "\n",
    "    \n",
    "    if len(frames) < desired_num_frames:\n",
    "        frames = np.concatenate([frames] * (desired_num_frames // len(frames) + 1), axis=0)\n",
    "        frames = frames[:desired_num_frames]\n",
    "    elif len(frames) > desired_num_frames:\n",
    "        frames = frames[:desired_num_frames]\n",
    "\n",
    "    frames = np.vstack(frames)  # Stack frames into a single array\n",
    "    return frames, label\n",
    "\n",
    "# Employs the feature extraction model to extract features from videos\n",
    "train_features = []\n",
    "val_features = []\n",
    "\n",
    "for file_path, label in zip(X_train, y_train):\n",
    "    frames, _ = load_and_preprocess_video(file_path, label)\n",
    "    if frames is not None:\n",
    "        features = feature_extraction_model.predict(frames)\n",
    "        train_features.append(features)\n",
    "\n",
    "for file_path, label in zip(X_val, y_val):\n",
    "    frames, _ = load_and_preprocess_video(file_path, label)\n",
    "    if frames is not None:\n",
    "        features = feature_extraction_model.predict(frames)\n",
    "        val_features.append(features)\n",
    "\n",
    "# Converts the extracted features to numpy arrays\n",
    "X_train_features = np.array(train_features).reshape(len(train_features), -1)\n",
    "X_val_features = np.array(val_features).reshape(len(val_features), -1)\n",
    "\n",
    "# Defines a hyperparameter search space for Random Search\n",
    "param_dist = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [100, 200, 300, 400, 500, 1000],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "}\n",
    "\n",
    "# Initializes the logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Initializes and runs RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    classifier,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Adjust the number of iterations as required\n",
    "    scoring='accuracy', \n",
    "    cv=5,  # Modifies the count of cross-validation folds as necessary.\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Teaches the random search about the data.\"\n",
    "random_search.fit(X_train_features, y_train)\n",
    "\n",
    "# Saves the optimized model to Google Drive directory\n",
    "joblib.dump(random_search.best_estimator_, '/content/drive/MyDrive/model_randomsearch.pkl')\n",
    "\n",
    "#Uses the best-guessing method from random search to make predictions on the validation data\n",
    "y_val_pred = random_search.best_estimator_.predict(X_val_features)\n",
    "\n",
    "# Calculates accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Calculates precision\n",
    "precision = precision_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Calculates ROC AUC\n",
    "y_val_proba = random_search.best_estimator_.predict_proba(X_val_features)[:, 1]\n",
    "roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "# Calculates recall\n",
    "recall = recall_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Calculates F1-score\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Creates and visualizes a confusion matrix heatmap\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['echt', 'df'], yticklabels=['echt', 'df'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Displays metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Accuracy: 0.73\n",
    "# Precision: 0.71\n",
    "# ROC AUC: 0.70\n",
    "# Recall: 0.89\n",
    "# F1-score: 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
