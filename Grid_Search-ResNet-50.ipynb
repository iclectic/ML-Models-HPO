{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load('/content/drive/MyDrive/model_filename_original_resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import joblib  # Import joblib for model saving\n",
    "import seaborn as sns  # Import seaborn for visualization\n",
    "import matplotlib.pyplot as plt  # Import matplotlib for visualization\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Define the base folder path to your dataset\n",
    "base_path = '/content/drive/MyDrive/Celeb-DFF'\n",
    "\n",
    "# Define subfolder paths for real and fake videos\n",
    "real_path_1 = os.path.join(base_path, 'Celeb-real')\n",
    "fake_path = os.path.join(base_path, 'Celeb-synthesis')\n",
    "\n",
    "# Collect file paths for real and fake videos\n",
    "fake_video_files = [os.path.join(fake_path, file) for file in os.listdir(fake_path) if file.endswith('.mp4')]\n",
    "real_video_files = [os.path.join(real_path_1, file) for file in os.listdir(real_path_1) if file.endswith('.mp4')]\n",
    "\n",
    "# Create labels\n",
    "real_labels = ['echt'] * len(real_video_files)\n",
    "fake_labels = ['df'] * len(fake_video_files)\n",
    "\n",
    "# Combine file paths and labels\n",
    "file_paths = real_video_files + fake_video_files\n",
    "labels = real_labels + fake_labels\n",
    "\n",
    "# Split the dataset into training (80%) and validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the pretrained ResNet-50 model with weights from ImageNet\n",
    "base_model = ResNet50(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Create the final feature extraction model\n",
    "feature_extraction_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Function to load and preprocess video frames with tqdm progress bars\n",
    "def load_and_preprocess_video(file_path, label, desired_num_frames=16):\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    frames = []\n",
    "\n",
    "    # Read frames from the video\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    for _ in tqdm(range(min(num_frames, desired_num_frames)), desc=f\"Processing {os.path.basename(file_path)}\", unit=\" frame\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize the frame to match ResNet-50 input size\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        frame = np.expand_dims(frame, axis=0)  # Add batch dimension\n",
    "\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Check if any frames were read\n",
    "    if not frames:\n",
    "        return None, label\n",
    "\n",
    "    # Pad or truncate frames to the desired number\n",
    "    if len(frames) < desired_num_frames:\n",
    "        frames = np.concatenate([frames] * (desired_num_frames // len(frames) + 1), axis=0)\n",
    "        frames = frames[:desired_num_frames]\n",
    "    elif len(frames) > desired_num_frames:\n",
    "        frames = frames[:desired_num_frames]\n",
    "\n",
    "    frames = np.vstack(frames)  # Stack frames into a single array\n",
    "    return frames, label\n",
    "\n",
    "# Use the feature extraction model to extract features from videos\n",
    "train_features = []\n",
    "val_features = []\n",
    "\n",
    "for file_path, label in zip(X_train, y_train):\n",
    "    frames, _ = load_and_preprocess_video(file_path, label)\n",
    "    if frames is not None:\n",
    "        features = feature_extraction_model.predict(frames)\n",
    "        train_features.append(features)\n",
    "\n",
    "for file_path, label in zip(X_val, y_val):\n",
    "    frames, _ = load_and_preprocess_video(file_path, label)\n",
    "    if frames is not None:\n",
    "        features = feature_extraction_model.predict(frames)\n",
    "        val_features.append(features)\n",
    "\n",
    "# Convert the extracted features to numpy arrays\n",
    "X_train_features = np.array(train_features).reshape(len(train_features), -1)\n",
    "X_val_features = np.array(val_features).reshape(len(val_features), -1)\n",
    "\n",
    "# Define an increased hyperparameter search space for Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [100, 200, 300, 400, 500, 1000],\n",
    "    'solver': ['lbfgs', 'liblinear', 'newton-cg'],\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "}\n",
    "\n",
    "# Initialize logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Initialize and run GridSearchCV\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=5, scoring='accuracy', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_features, y_train)\n",
    "\n",
    "# Save the optimized model to your Google Drive directory\n",
    "joblib.dump(grid_search.best_estimator_, '/content/drive/MyDrive/grid_search_optimized.pkl')\n",
    "\n",
    "# Predictions on validation data using the best estimator from grid search\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val_features)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Calculate ROC AUC\n",
    "y_val_proba = grid_search.best_estimator_.predict_proba(X_val_features)[:, 1]\n",
    "roc_auc = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(y_val, y_val_pred, pos_label='df')\n",
    "\n",
    "# Create and visualize a confusion matrix heatmap\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['echt', 'df'], yticklabels=['echt', 'df'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of iterations (max_iter) or scale the data as shown in:\n",
    "#     https://scikit-learn.org/stable/modules/preprocessing.html\n",
    "# Please also refer to the documentation for alternative solver options:\n",
    "#     https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "#   n_iter_i = _check_optimize_result(\n",
    "\n",
    "# Accuracy: 0.73\n",
    "# Precision: 0.70\n",
    "# ROC AUC: 0.70\n",
    "# Recall: 0.91\n",
    "# F1-score: 0.79"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
